{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SethCram/CS474-Deep-Learning/blob/main/HW4/HW4_CNN_Cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2MIMrBnHj6D"
      },
      "outputs": [],
      "source": [
        "import keras \n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import applications\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2 as cv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Google Colab Cell\n",
        "\n",
        "#enable debugging\n",
        "!pip install -Uqq ipdb\n",
        "import ipdb\n",
        "%pdb on\n",
        "\n",
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "training_imgs_path = '/content/drive/MyDrive/School/Senior Year/CS 474-01 (Deep Learning)/trainImgs'\n",
        "test_imgs_path = '/content/drive/MyDrive/School/Senior Year/CS 474-01 (Deep Learning)/testImgs'"
      ],
      "metadata": {
        "id": "1dQmHIT2Mtvz",
        "outputId": "641f8dfe-7d32-4241-af7b-f499d9badde8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 793 kB 6.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 41.8 MB/s \n",
            "\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n",
            "Automatic pdb calling has been turned ON\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi3lNvXkHj6E",
        "outputId": "b8ce112f-fa2a-4f1d-9bf3-ead03af9ef6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Unnamed: 0         img name  tumor types\n",
            "0              0           000137            1\n",
            "1              1         case0419            1\n",
            "2              2         case0024            0\n",
            "3              3     benign (397)            0\n",
            "4              4         case0287            0\n",
            "...          ...              ...          ...\n",
            "2379        2379  malignant (143)            1\n",
            "2380        2380         case0441            1\n",
            "2381        2381     benign (162)            0\n",
            "2382        2382           000064            0\n",
            "2383        2383           000022            0\n",
            "\n",
            "[2384 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6ef196490242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# store training and test imgs greyscaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m x_train_diff_reses = np.array([\n\u001b[0;32m----> 7\u001b[0;31m         cv.imread(\"trainImgs/\" + row[1] + \".png\", 0).astype('float32') for row in df.values], \n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-7-6ef196490242>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# store training and test imgs greyscaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m x_train_diff_reses = np.array([\n\u001b[0;32m----> 7\u001b[0;31m         cv.imread(\"trainImgs/\" + row[1] + \".png\", 0).astype('float32') for row in df.values], \n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-7-6ef196490242>\u001b[0m(7)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m      5 \u001b[0;31m\u001b[0;31m# store training and test imgs greyscaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      6 \u001b[0;31mx_train_diff_reses = np.array([\n",
            "\u001b[0m\u001b[0;32m----> 7 \u001b[0;31m        cv.imread(\"trainImgs/\" + row[1] + \".png\", 0).astype('float32') for row in df.values], \n",
            "\u001b[0m\u001b[0;32m      8 \u001b[0;31m        \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      9 \u001b[0;31m    )\n",
            "\u001b[0m\n",
            "ipdb> exit()\n"
          ]
        }
      ],
      "source": [
        "#Load data\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/SethCram/CS474-Deep-Learning/main/HW4/train.csv', sep = ',')\n",
        "print(df)\n",
        "    \n",
        "# store training and test imgs greyscaled \n",
        "x_train_diff_reses = np.array([\n",
        "        cv.imread(\"trainImgs/\" + row[1] + \".png\", 0).astype('float32') for row in df.values], \n",
        "        dtype='object'\n",
        "    )\n",
        "x_test_diff_reses = np.array([\n",
        "        cv.imread(\"testImgs/\" + str(i) + \".png\", 0).astype('float32') \n",
        "        for i in range(1248)], \n",
        "        dtype='object'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILcFYq4tHj6F",
        "outputId": "9b7cb0b2-0fe1-4f81-b702-1e8067b9118c"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [4], line 108\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[39m#x_train = np.reshape(x_train, [x_train.shape[0], x_train.shape[1] * x_train.shape[2]]) #adds 2nd dim of 1\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[39m#x_test = np.reshape(x_test, [x_test.shape[0], x_test.shape[1] * x_test.shape[2]]) #adds 2nd dim of 1\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m x_train, y_train, x_test, y_test\n\u001b[1;32m--> 108\u001b[0m x_train, y_train, x_test, y_test \u001b[39m=\u001b[39m load_imgs()\n\u001b[0;32m    110\u001b[0m \u001b[39mpass\u001b[39;00m\n",
            "Cell \u001b[1;32mIn [4], line 76\u001b[0m, in \u001b[0;36mload_imgs\u001b[1;34m(show_sample, batch_size)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i, image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x_train_diff_reses):\n\u001b[0;32m     61\u001b[0m     \u001b[39m#x_train_diff_reses[i] = pad_image(\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m#    image, \u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[39m#    biggest_training_width, \u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39m#    biggest_training_height\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m#)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     x_train_diff_reses[i] \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mcopyMakeBorder(\n\u001b[0;32m     68\u001b[0m         image, \n\u001b[0;32m     69\u001b[0m         \u001b[39m0\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m         cv\u001b[39m.\u001b[39mBORDER_CONSTANT\n\u001b[0;32m     74\u001b[0m     )\n\u001b[1;32m---> 76\u001b[0m     plt\u001b[39m.\u001b[39mplot(),plt\u001b[39m.\u001b[39mimshow(x_train_diff_reses[i],\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m),plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mORIGINAL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m m \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x_train_diff_reses)\n\u001b[0;32m     79\u001b[0m x_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((\u001b[39mlen\u001b[39m()))\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\matplotlib\\pyplot.py:2611\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2605\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2607\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2608\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m   2609\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[0;32m   2610\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2611\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39mimshow(\n\u001b[0;32m   2612\u001b[0m         X, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm, aspect\u001b[39m=\u001b[39maspect,\n\u001b[0;32m   2613\u001b[0m         interpolation\u001b[39m=\u001b[39minterpolation, alpha\u001b[39m=\u001b[39malpha, vmin\u001b[39m=\u001b[39mvmin,\n\u001b[0;32m   2614\u001b[0m         vmax\u001b[39m=\u001b[39mvmax, origin\u001b[39m=\u001b[39morigin, extent\u001b[39m=\u001b[39mextent,\n\u001b[0;32m   2615\u001b[0m         interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   2616\u001b[0m         filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   2617\u001b[0m         url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[0;32m   2618\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2619\u001b[0m     sci(__ret)\n\u001b[0;32m   2620\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\matplotlib\\__init__.py:1423\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1422\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1423\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1425\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1426\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1427\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5572\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5564\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5565\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[0;32m   5566\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[0;32m   5567\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[0;32m   5568\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   5569\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5570\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5572\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5573\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5574\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5575\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\matplotlib\\image.py:697\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(A, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[0;32m    696\u001b[0m     A \u001b[39m=\u001b[39m pil_to_array(A)  \u001b[39m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39;49msafe_masked_invalid(A, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[0;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:743\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[1;34m(x, copy)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_masked_invalid\u001b[39m(x, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 743\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(x, subok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    744\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39misnative:\n\u001b[0;32m    745\u001b[0m         \u001b[39m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[0;32m    746\u001b[0m         \u001b[39m# copy with the byte order swapped.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mbyteswap(inplace\u001b[39m=\u001b[39mcopy)\u001b[39m.\u001b[39mnewbyteorder(\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Swap to native order.\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in callback <function flush_figures at 0x0000026FC36D2B00> (for post_execute):\n"
          ]
        }
      ],
      "source": [
        "#Get training and test max resolutions\n",
        "\n",
        "biggest_training_width = max( image.shape[0] for image in x_train_diff_reses )\n",
        "biggest_training_height = max( image.shape[1] for image in x_train_diff_reses )\n",
        "\n",
        "biggest_test_width = max( image.shape[0] for image in x_test_diff_reses )\n",
        "biggest_test_height = max( image.shape[1] for image in x_test_diff_reses )\n",
        "\n",
        "#preprocess_input = keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "## load the cancer dataset\n",
        "def construct_image_batch(image_group):\n",
        "    \n",
        "    # get the max image shape\n",
        "    max_shape = tuple(max(image.shape[x] for image in image_group) for x in range(2))\n",
        "\n",
        "    # construct an image batch object\n",
        "    image_batch = np.zeros((len(image_group),) + max_shape, dtype='float32')\n",
        "\n",
        "    # copy all images to the upper left part of the image batch object\n",
        "    for image_index, image in enumerate(image_group):\n",
        "        image_batch[image_index, :image.shape[0], :image.shape[1]] = image\n",
        "\n",
        "    return image_batch\n",
        "\n",
        "def pad_image(image, maxX, maxY):\n",
        "    #apply zero padding to smaller images\n",
        "    zero_image = np.zeros(\n",
        "        (maxX, maxY),\n",
        "         dtype='float32'\n",
        "    )\n",
        "    # copy image to the upper left part of zero image\n",
        "    zero_image[:image.shape[0], :image.shape[1]] = image\n",
        "    \n",
        "    return zero_image\n",
        "\n",
        "def load_imgs(show_sample = True, batch_size=100):\n",
        "    \n",
        "    ## (1) Data preparation\n",
        "    #df=pd.read_csv('train.csv', sep = ',')\n",
        "    #print(df)\n",
        "    \n",
        "    #x_test = np.empty((2384, 988, 988))\n",
        "    \n",
        "    #for i in range(x_test.shape[0]):\n",
        "    #    x_test[i] = cv.imread(\"testImgs/\" + str(i) + \".png\", 0)\n",
        "    \n",
        "    #need to do some sort of zero/same padding to get same image sizes\n",
        "    # unless using fully convolutional NN bc can take inputs of different sizes\n",
        "    # or if use a spatial pyramid pooling (SPP) layer before dense layers\n",
        "    \n",
        "    #Could possibly create more training data thru upscaling/downscaling\n",
        "    # would better recognise diff scaled data\n",
        "    \n",
        "    # store training and test imgs greyscaled #could greyscale images for faster proccing\n",
        "    #x_train_diff_reses = np.array([cv.imread(\"trainImgs/\" + row[1] + \".png\", 0).astype('float32') for row in df.values], dtype='object')\n",
        "    \n",
        "    #resolutions = np.array(resolutions).T\n",
        "    \n",
        "    for i, image in enumerate(x_train_diff_reses):\n",
        "        #x_train_diff_reses[i] = pad_image(\n",
        "        #    image, \n",
        "        #    biggest_training_width, \n",
        "        #    biggest_training_height\n",
        "        #)\n",
        "        \n",
        "        x_train_diff_reses[i] = cv.copyMakeBorder(\n",
        "            image, \n",
        "            0, \n",
        "            biggest_training_height,#-image.shape[0], \n",
        "            0, \n",
        "            biggest_training_width,#-image.shape[1], \n",
        "            cv.BORDER_CONSTANT\n",
        "        )\n",
        "    \n",
        "        plt.plot()\n",
        "        plt.imshow(x_train_diff_reses[i],'gray')\n",
        "    \n",
        "    m = len(x_train_diff_reses)\n",
        "    x_train = np.empty((len()))\n",
        "    \n",
        "    for b in range(int(m/batch_size)):\n",
        "        b_start= b*batch_size\n",
        "        b_end = min((b+1)*batch_size, m)\n",
        "        \n",
        "        x_batch = x_train_diff_reses[b_start:b_end]\n",
        "        x_train[b_start:b_end] = construct_image_batch(x_batch)\n",
        "    \n",
        "    #x_test = np.array([cv.imread(\"testImgs/\" + str(i) + \".png\", 0).astype('float32') for i in range(1248)], dtype='object')\n",
        "    #shapes of both are messed up\n",
        "    \n",
        "    #store whether benign (0?) or malignant (1?)\n",
        "    y_train = np.array([row[2] for row in df.values], dtype=object)\n",
        "    y_test = 0 #no labels on test data??\n",
        "    \n",
        "    #show first 100 images\n",
        "    if show_sample == True:\n",
        "        nImg = 4\n",
        "        for i in range(nImg*nImg):\n",
        "            plt.subplot(nImg, nImg, i+1)\n",
        "            plt.imshow(x_train[i], cmap = 'Greys_r')\n",
        "        plt.show()\n",
        "        \n",
        "    #x_train = np.reshape(x_train, [x_train.shape[0], x_train.shape[1] * x_train.shape[2]]) #adds 2nd dim of 1\n",
        "    #x_test = np.reshape(x_test, [x_test.shape[0], x_test.shape[1] * x_test.shape[2]]) #adds 2nd dim of 1\n",
        "        \n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_imgs()\n",
        "\n",
        "pass\n",
        "\n",
        "#print('Data shape:', 'x_train:', x_train.shape, 'x_test:', x_test.shape)\n",
        "#print('Data shape:', 'y_train:', y_train.shape, 'y_test:', y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CDsQ12PHj6F",
        "outputId": "7197a498-b39f-4ea4-e7c9-d4e4a43bd8a2"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "`input_shape` must be a tuple of three integers.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# expand dims for channels?\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m applications\u001b[39m.\u001b[39mresnet_v2\u001b[39m.\u001b[39mResNet101V2(\n\u001b[0;32m      4\u001b[0m     include_top\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     input_tensor\u001b[39m=\u001b[39mx_train,\n\u001b[0;32m      7\u001b[0m     input_shape\u001b[39m=\u001b[39mx_train\u001b[39m.\u001b[39mshape,\n\u001b[0;32m      8\u001b[0m     pooling\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     classes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     10\u001b[0m     classifier_activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m desiredLevels \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mCreateConvLayer\u001b[39m(convLayers, model, filters, kernel_size, activation \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, padding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\keras\\applications\\resnet_v2.py:85\u001b[0m, in \u001b[0;36mResNet101V2\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m     82\u001b[0m     x \u001b[39m=\u001b[39m resnet\u001b[39m.\u001b[39mstack2(x, \u001b[39m256\u001b[39m, \u001b[39m23\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m     \u001b[39mreturn\u001b[39;00m resnet\u001b[39m.\u001b[39mstack2(x, \u001b[39m512\u001b[39m, \u001b[39m3\u001b[39m, stride1\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[39mreturn\u001b[39;00m resnet\u001b[39m.\u001b[39;49mResNet(\n\u001b[0;32m     86\u001b[0m     stack_fn,\n\u001b[0;32m     87\u001b[0m     \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     88\u001b[0m     \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     89\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mresnet101v2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     90\u001b[0m     include_top,\n\u001b[0;32m     91\u001b[0m     weights,\n\u001b[0;32m     92\u001b[0m     input_tensor,\n\u001b[0;32m     93\u001b[0m     input_shape,\n\u001b[0;32m     94\u001b[0m     pooling,\n\u001b[0;32m     95\u001b[0m     classes,\n\u001b[0;32m     96\u001b[0m     classifier_activation\u001b[39m=\u001b[39;49mclassifier_activation,\n\u001b[0;32m     97\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\keras\\applications\\resnet.py:159\u001b[0m, in \u001b[0;36mResNet\u001b[1;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    154\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top`\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m as true, `classes` should be 1000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[0;32m    160\u001b[0m     input_shape,\n\u001b[0;32m    161\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m224\u001b[39;49m,\n\u001b[0;32m    162\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m    163\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[0;32m    164\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[0;32m    165\u001b[0m     weights\u001b[39m=\u001b[39;49mweights,\n\u001b[0;32m    166\u001b[0m )\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:397\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39mif\u001b[39;00m input_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(input_shape) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`input_shape` must be a tuple of three integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         )\n\u001b[0;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m weights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    402\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe input must have 3 channels; Received \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m         )\n",
            "\u001b[1;31mValueError\u001b[0m: `input_shape` must be a tuple of three integers."
          ]
        }
      ],
      "source": [
        "# expand dims for channels?\n",
        "\n",
        "#model = applications.resnet_v2.ResNet101V2(\n",
        "#    include_top=False,\n",
        "#    weights='imagenet',\n",
        "#    input_tensor=x_train,\n",
        "#    input_shape=x_train.shape,\n",
        "#    pooling=None,\n",
        "#    classes=1,\n",
        "#    classifier_activation='softmax'\n",
        "#)\n",
        "\n",
        "desiredLevels = 2\n",
        "\n",
        "def CreateConvLayer(convLayers, model, filters, kernel_size, activation = None, padding = \"valid\"):\n",
        "    conv = layers.Conv2D(\n",
        "        filters= filters, #num of filters for conv\n",
        "        kernel_size = kernel_size, \n",
        "        padding = padding,\n",
        "        activation = activation,\n",
        "        #input_shape = (28, 28, 1) #only 28 params \n",
        "    )\n",
        "    \n",
        "    convLayers.append(conv)\n",
        "    model.add(conv)\n",
        "    \n",
        "def CreateMaxPoolLayer(poolLayers, model, pool_size, strides):\n",
        "    pool = layers.MaxPooling2D(\n",
        "        pool_size = pool_size,\n",
        "        strides = strides,\n",
        "    )\n",
        "    \n",
        "    poolLayers.append(pool)\n",
        "    model.add(pool)\n",
        "    \n",
        "def CreateConvBlock(\n",
        "    model, convLayers, poolLayers, normLayers, activationLayers,\n",
        "    filters, kernel_size, activation,\n",
        "    pool_size, strides\n",
        "):\n",
        "    CreateConvLayer(convLayers, model, filters, kernel_size)\n",
        "    #CreateMaxPoolLayer(poolLayers, model, pool_size, strides)\n",
        "    \n",
        "    dropout = layers.Dropout(0.2)\n",
        "    model.add(dropout)\n",
        "    \n",
        "    norm = layers.BatchNormalization()\n",
        "    model.add(norm)\n",
        "    normLayers.append(norm)\n",
        "\n",
        "    activ = layers.Activation(activation)\n",
        "    model.add(activ)\n",
        "    activationLayers.append(activ)\n",
        "    \n",
        "# Create a fully conv NN\n",
        "model_fcn = keras.Sequential()\n",
        "\n",
        "convLayers = []\n",
        "poolLayers = []\n",
        "normLayers = []\n",
        "activationLayers = []\n",
        "\n",
        "# Input layer\n",
        "input = layers.Input(shape=(None, None, 1))\n",
        "model_fcn.add(input)\n",
        "\n",
        "CreateConvBlock(\n",
        "    model_fcn, convLayers, poolLayers, normLayers, activationLayers,\n",
        "    filters=8, kernel_size=3, activation='relu',\n",
        "    pool_size=2, strides=2\n",
        ")\n",
        "\n",
        "CreateConvBlock(\n",
        "    model_fcn, convLayers, poolLayers, normLayers, activationLayers,\n",
        "    filters=16, kernel_size=3, activation='softmax',\n",
        "    pool_size=2, strides=2\n",
        ")\n",
        "\n",
        "globMaxPool = layers.GlobalMaxPooling2D()\n",
        "model_fcn.add(globMaxPool)\n",
        "\n",
        "#model_fcn(inputs=x_train, outputs=y_train)\n",
        "\n",
        "#need normalization and output layers\n",
        "\n",
        "#specify optimization\n",
        "model_fcn.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_fcn.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5uxl6ePHj6G",
        "outputId": "4944fa13-c62c-4945-b6b5-d8e11eecf5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_20 (InputLayer)       [(None, 231, 231, 1)]     0         \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 229, 229, 128)     1280      \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 229, 229, 128)     0         \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 229, 229, 128)    512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_42 (Activation)  (None, 229, 229, 128)     0         \n",
            "                                                                 \n",
            " conv2d_51 (Conv2D)          (None, 229, 229, 2)       258       \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 229, 229, 2)       0         \n",
            "                                                                 \n",
            " batch_normalization_50 (Bat  (None, 229, 229, 2)      8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_max_pooling2d_19 (Gl  (None, 2)                0         \n",
            " obalMaxPooling2D)                                               \n",
            "                                                                 \n",
            " activation_43 (Activation)  (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,058\n",
            "Trainable params: 1,798\n",
            "Non-trainable params: 260\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def construct_image_batch(image_group, BATCH_SIZE):\n",
        "    # get the max image shape\n",
        "    max_shape = tuple(max(image.shape[x] for image in image_group) for x in range(3))\n",
        "\n",
        "    # construct an image batch object\n",
        "    image_batch = np.zeros((BATCH_SIZE,) + max_shape, dtype='float32')\n",
        "\n",
        "    # copy all images to the upper left part of the image batch object\n",
        "    for image_index, image in enumerate(image_group):\n",
        "        image_batch[image_index, :image.shape[0], :image.shape[1], :image.shape[2]] = image\n",
        "\n",
        "    return image_batch\n",
        "\n",
        "classes = 2\n",
        "\n",
        "# Input layer\n",
        "input = layers.Input(shape=(231, 231, 1)) #test with smallest input shape to determine # of conv blocks?\n",
        "\n",
        "# input convolution block\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, strides=1)(input)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)\n",
        "\n",
        "# middle convolution block\n",
        "#x = layers.Conv2D(filters=64, kernel_size=3, strides=1)(x)\n",
        "#x = layers.Dropout(0.2)(x)\n",
        "#x = layers.BatchNormalization()(x)\n",
        "#x = layers.Activation('relu')(x)\n",
        "\n",
        "# middle convolution block\n",
        "#x = layers.Conv2D(filters=32, kernel_size=3, strides=1)(x)\n",
        "#x = layers.Dropout(0.2)(x)\n",
        "#x = layers.BatchNormalization()(x)\n",
        "#x = layers.Activation('relu')(x)\n",
        "\n",
        "# final convolution block\n",
        "x = layers.Conv2D(filters=classes, kernel_size=1, strides=1)(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.GlobalMaxPooling2D()(x)\n",
        "predictions = layers.Activation('relu')(x)\n",
        "\n",
        "model = keras.Model(inputs=input, outputs=predictions)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO5uAwL4Hj6G",
        "outputId": "83dc57e0-a9af-4ec2-989c-03009f8ee725"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model_fcn\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m     x_train, \u001b[39m#should be 2d inputs\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m     y_train, \u001b[39m#already onehot\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
            "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
          ]
        }
      ],
      "source": [
        "def train(model, train_generator, val_generator, epochs = 50):\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "    checkpoint_path = './snapshots'\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    model_path = os.path.join(checkpoint_path, 'model_epoch_{epoch:02d}_loss_{loss:.2f}_acc_{acc:.2f}_val_loss_{val_loss:.2f}_val_acc_{val_acc:.2f}.h5')\n",
        "    \n",
        "    history = model.fit_generator(generator=train_generator,\n",
        "            steps_per_epoch=len(train_generator),\n",
        "            epochs=epochs,\n",
        "            callbacks=[keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)],\n",
        "            validation_data=val_generator,\n",
        "            validation_steps=len(val_generator)\n",
        "        )\n",
        "\n",
        "    return history\n",
        "\n",
        "#train model\n",
        "history = model_fcn.fit(\n",
        "    x_train, #should be 2d inputs\n",
        "    y_train, #already onehot\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HdsbyQRHj6H"
      },
      "outputs": [],
      "source": [
        "#test accuracy\n",
        "y_test_pred = model_fcn.predict(x_test)\n",
        "y_test_pred = np.argmax(y_test_pred, axis=1) #still need to do bc pred is prob?\n",
        "test_err = np.sum(y_test == y_test_pred) / y_test.shape[0]\n",
        "\n",
        "print(f'test accuracy: {test_err * 100}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKaKEEa0Hj6H"
      },
      "outputs": [],
      "source": [
        "#history of training and validation accuracu\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['training acc', 'validation acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2J8oG7QHj6H"
      },
      "outputs": [],
      "source": [
        "#get access to filters/kernels\n",
        "weights = convLayers[0].get_weights()[0][:, :, 0, :] #arr of a buncha nested lists, 0 bc only have 1 channel (28, 28, 1, 8)\n",
        "\n",
        "#can get access to other params this way \n",
        "\n",
        "for i in range(1, 8):\n",
        "    plt.subplot(2, 4, i)\n",
        "    plt.imshow(weights[:,:,i], cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "#how get access to feature maps?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 ('JupyterNotebook': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7e1b2f627b942c46ec4bf47ff4ea2a9cdb5031d2bd74349327a6cc0e56088180"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}