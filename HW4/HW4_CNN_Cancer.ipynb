{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/\n",
        "https://www.tensorflow.org/tutorials/images/transfer_learning#fine_tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Colab\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/SethCram/CS474-Deep-Learning/blob/main/HW4/HW4_CNN_Cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dQmHIT2Mtvz",
        "outputId": "09cc133b-10f4-444f-915d-a129dc8b00dd"
      },
      "outputs": [],
      "source": [
        "## Google Colab Cell\n",
        "\n",
        "#enable debugging\n",
        "!pip install -Uqq ipdb\n",
        "import ipdb\n",
        "%pdb on\n",
        "\n",
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#seth act\n",
        "%cd /content/drive/MyDrive/School/Senior Year/CS 474-01 (Deep Learning)/\n",
        "#other act\n",
        "%cd /content/drive/MyDrive/DL-Proj4/\n",
        "\n",
        "#make sure GPU enabled\n",
        "!nvidia-smi\n",
        "\n",
        "#install proper TF version for convnext\n",
        "!pip install tensorflow==2.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2MIMrBnHj6D"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import convnext\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import applications\n",
        "from keras import losses\n",
        "from keras import callbacks\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2 as cv\n",
        "import csv\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "\n",
        "print(\"Modules imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi3lNvXkHj6E",
        "outputId": "59eb91eb-b9e2-49d9-9a6d-e01836dffe24"
      },
      "outputs": [],
      "source": [
        "#LOAD CSV AND IMAGES (in proper resolution)\n",
        "\n",
        "df=pd.read_csv('train.csv', sep = ',')\n",
        "print(df)\n",
        "\n",
        "training_imgs_path = 'trainImgs/'\n",
        "test_imgs_path = 'testImgs/'\n",
        "\n",
        "targetWidth = 512 #500 #256 for imagenet\n",
        "targetHeight = 512 #500\n",
        "\n",
        "# store training and test imgs greyscaled \n",
        "x_train = np.array([\n",
        "            cv.resize( #resize to desired size\n",
        "                cv.imread(training_imgs_path + row[1] + \".png\"), \n",
        "                dsize=(targetWidth, targetHeight),\n",
        "            ) \n",
        "            for row in df.values\n",
        "    ])\n",
        "x_test = np.array([\n",
        "            cv.resize( #resize to desired size\n",
        "                cv.imread(test_imgs_path + str(i) + \".png\"),\n",
        "                dsize=(targetWidth, targetHeight),\n",
        "            )\n",
        "            for i in range(1248)\n",
        "    ])\n",
        "\n",
        "#store whether benign (0?) or malignant (1?)\n",
        "y_train = np.array([row[2] for row in df.values], dtype='float32')\n",
        "\n",
        "nImg = 4  \n",
        "for i in range(nImg*nImg):\n",
        "    plt.subplot(nImg, nImg, i+1)\n",
        "    plt.imshow(x_train[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ANOTHER WAY TO IMPORT DATA (possibly faster)\n",
        "\n",
        "# load all training images and labels\n",
        "train_img_folder = 'trainImgs'\n",
        "train = pd.read_csv('train.csv')\n",
        "print(train)\n",
        "\n",
        "targetWidth = 512 \n",
        "targetHeight = 512 \n",
        "input_size = 512\n",
        "\n",
        "trainImg_names = list(train['img name']) \n",
        "print('loading images ...')\n",
        "x_train = []\n",
        "for idx, name in enumerate(trainImg_names):\n",
        "    img = image.load_img(os.path.join(train_img_folder, name +'.png'), target_size = (input_size, input_size)) #color_mode = \"grayscale\",\n",
        "    img = image.img_to_array(img)\n",
        "    x_train.append(img)\n",
        "    #print(idx, name)\n",
        "x_train = np.array(x_train).astype(np.float32)\n",
        "y_train = np.array(train['tumor types'])\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "#y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
        "#print(y_train_onehot.shape)\n",
        "\n",
        "print('loading finished ...')\n",
        "#print(x_train.shape, y_train_onehot)\n",
        "\n",
        "nImg = 4  \n",
        "for i in range(nImg*nImg):\n",
        "    plt.subplot(nImg, nImg, i+1)\n",
        "    plt.imshow(x_train[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transfer Learning\n",
        "## Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comon Model Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_layer = layers.Input(shape=(targetWidth, targetHeight, 3))\n",
        "\n",
        "#data augmentation\n",
        "flip_layer = layers.RandomFlip()\n",
        "rot_layer = layers.RandomRotation( #seems to throw off models and slow down training\n",
        "        0.2,\n",
        "        fill_mode=\"constant\",\n",
        "        fill_value=0,\n",
        "    )\n",
        "translation_layer = layers.RandomTranslation(\n",
        "    height_factor=(-0.2, 0.2),\n",
        "    width_factor=(-0.2, 0.2),\n",
        "    fill_mode=\"constant\",\n",
        "    fill_value=0,\n",
        ")\n",
        "\n",
        "output_layer = layers.Dense(\n",
        "    units=1,\n",
        "    activation=keras.activations.sigmoid \n",
        ")\n",
        "\n",
        "nImg = 4  \n",
        "for i in range(nImg*nImg):\n",
        "    plt.subplot(nImg, nImg, i+1)\n",
        "    \n",
        "    aug_image = flip_layer(x_train[i])\n",
        "    aug_image = translation_layer(aug_image)\n",
        "    aug_image = rot_layer(aug_image)\n",
        "    \n",
        "    plt.imshow(aug_image.numpy().astype(\"uint8\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Model Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = flip_layer(input_layer)\n",
        "#x = rot_layer(x)\n",
        "#x = translation_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VGG Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## VGG\n",
        "\n",
        "filename = \"vgg16_firstpass_retry_model\"\n",
        "\n",
        "#avg = 530\n",
        "#avg_training_width = sum( image.shape[0] for image in x_train ) / x_train.shape[0]\n",
        "#avg_training_height = sum( image.shape[1] for image in x_train ) / x_train.shape[0]\n",
        "#avg_test_width = sum( image.shape[0] for image in x_test ) / x_test.shape[0]\n",
        "#avg_test_height = sum( image.shape[1] for image in x_train ) / x_train.shape[0]\n",
        "\n",
        "# layer construction\n",
        "preprocc_input = applications.vgg16.preprocess_input\n",
        "base_model = applications.VGG16(\n",
        "    input_shape=(targetWidth, targetHeight, 3),\n",
        "    include_top=False,\n",
        ")\n",
        "flatten_layer = layers.Flatten()\n",
        "fc_layer = layers.Dense(512, activation='relu')\n",
        "# Add a dropout rate of 0.5\n",
        "dropout_layer = layers.Dropout(0.5)\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "#layer connecting\n",
        "x = preprocc_input(x) #training should be true here?\n",
        "x = base_model(x, training=False)\n",
        "x = flatten_layer(x)\n",
        "x = fc_layer(x)\n",
        "x = dropout_layer(x)\n",
        "predictions = output_layer(x)\n",
        "model = keras.Model(input_layer, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### InceptionV3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"inceptronv3_firstpass_retry_model\"\n",
        "\n",
        "# layer construction\n",
        "preprocc_input = applications.inception_v3.preprocess_input\n",
        "\n",
        "base_model = applications.InceptionV3(\n",
        "    input_shape=(targetWidth, targetHeight, 3),\n",
        "    include_top=False,\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "global_avg_pool_layer = layers.GlobalAveragePooling2D()\n",
        "fc_layer = layers.Dense(1024, activation='relu')\n",
        "dropout_layer = layers.Dropout(0.2)\n",
        "\n",
        "#layer connecting\n",
        "x = preprocc_input(x) #training should be true here?\n",
        "x = base_model(x, training=False)\n",
        "x = global_avg_pool_layer(x)\n",
        "x = fc_layer(x)\n",
        "x = dropout_layer(x)\n",
        "predictions = output_layer(x)\n",
        "model = keras.Model(input_layer, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Efficient Net Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"EffNetB2_firstpass_model\"\n",
        "\n",
        "# layer construction\n",
        "preprocc_input = applications.efficientnet.preprocess_input\n",
        "base_model = applications.EfficientNetB2(\n",
        "    input_shape=(targetWidth, targetHeight, 3),\n",
        "    include_top=False,\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "flatten_layer = layers.Flatten()\n",
        "fc_layer = layers.Dense(512, activation='relu')\n",
        "dropout_layer = layers.Dropout(0.3) \n",
        "\n",
        "#layer connecting\n",
        "x = preprocc_input(x) #training should be true here?\n",
        "x = base_model(x, training=False)\n",
        "x = flatten_layer(x)\n",
        "x = fc_layer(x)\n",
        "x = dropout_layer(x)\n",
        "predictions = output_layer(x)\n",
        "model = keras.Model(input_layer, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConvNext Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"ConvNextTiny_finetune_firstpass_model\"\n",
        "\n",
        "# layer construction\n",
        "base_model = convnext.ConvNeXtTiny( #preproccing included\n",
        "    input_shape=(targetWidth, targetHeight, 3),\n",
        "    include_top=False,\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "flatten_layer = layers.Flatten()\n",
        "#fc_layer = layers.Dense(216, activation='relu')\n",
        "#dropout_layer = layers.Dropout(0.3) \n",
        "\n",
        "#layer connecting\n",
        "x = base_model(x, training=False)\n",
        "x = flatten_layer(x)\n",
        "#x = fc_layer(x)\n",
        "#x = dropout_layer(x)\n",
        "predictions = output_layer(x)\n",
        "model = keras.Model(input_layer, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MobileNet V2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"mobilenetv2_firstpass_finetuned3_model\"\n",
        "\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=(targetWidth, targetHeight, 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "base_model.trainable = False\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "prediction_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "#layer connecting\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(input_layer, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HuggingFace Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## UNTRAINABLE\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "filename = \"ENM/sciBERT-case-finetuned-breastcancer\"\n",
        "num_classes = 1\n",
        "batch_size = 32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(filename)\n",
        "\n",
        "model = TFAutoModel.from_pretrained(filename, from_pt=True, num_labels=num_classes)\n",
        "#model.trainable = True\n",
        "\n",
        "model.summary(expand_nested=True, show_trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_learning_rate = 0.0001\n",
        "num_epochs = 10\n",
        "\n",
        "num_train_steps = len(x_train) * num_epochs\n",
        "\n",
        "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=base_learning_rate, end_learning_rate=0.0,\n",
        "        decay_steps=num_train_steps\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=optimizers.Adam(), #learning_rate=lr_scheduler\n",
        "    loss=losses.BinaryCrossentropy(), #from_logits=True\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=optimizers.RMSprop(), #optimizers.rmsprop(lr=0.0001, decay=1e-6) #sometimes decay not included?\n",
        "    loss=losses.BinaryCrossentropy(), \n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#need to recompile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.SGD(learning_rate=base_learning_rate, momentum=0.9), \n",
        "    loss=losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#need to recompile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.RMSprop(learning_rate=base_learning_rate/10), \n",
        "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Post Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary(expand_nested=True, show_trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Restoration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restore Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## RESTORE THE MODEL\n",
        "\n",
        "filename = \"inceptronv3_dropout3_finetuned2.1_model\"\n",
        "\n",
        "model = keras.models.load_model(\"saved_models/\" + filename)\n",
        "\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "#filename = \"inceptronv3_dropout3_finetuned2.1_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"inceptronv3_dropout3_finetuned_model\"\n",
        "\n",
        "model = keras.models.load_model(\"checkpoints/\" + filename)\n",
        "\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "filename = \"inceptronv3_dropout3_finetuned2.1_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine Tuning\n",
        "\n",
        "shouldn't be finetuning if additional dense layer before output layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 249 layers and unfreeze the rest:\n",
        "\n",
        "#base_model = model.layers[3]\n",
        "\n",
        "base_model_index = 5\n",
        "\n",
        "#enable base model training\n",
        "model.layers[base_model_index ].trainable = True\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "#fine_tune_at = 100\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "#for layer in model.layers[3].layers[:fine_tune_at]:\n",
        "#  layer.trainable = False\n",
        "\n",
        "#disable all base model layers\n",
        "for base_model_layer in model.layers[base_model_index ].layers:\n",
        "    base_model_layer.trainable = False\n",
        "\n",
        "#make some conv2D layers trainable\n",
        "model.layers[base_model_index ].layers[299].trainable = True\n",
        "\n",
        "model.layers[base_model_index ].layers[294].trainable = True\n",
        "\n",
        "model.layers[base_model_index].layers[292].trainable = True\n",
        "model.layers[base_model_index].layers[291].trainable = True\n",
        "model.layers[base_model_index].layers[290].trainable = True\n",
        "model.layers[base_model_index].layers[289].trainable = True\n",
        "\n",
        "model.layers[base_model_index].layers[284].trainable = True\n",
        "model.layers[base_model_index].layers[283].trainable = True\n",
        "\n",
        "\n",
        "model.summary(expand_nested=True, show_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.layers[5].trainable = True\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 50\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in model.layers[5].layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "  \n",
        "model.summary(expand_nested=True, show_trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#filename = \"mobilenetv2_firstpass_finetuned3large2_model\"\n",
        "\n",
        "#shuffle data everytime train\n",
        "x_train, y_train = shuffle(x_train, y_train)\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "early_stopping_callback = callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    min_delta=0.001,\n",
        "    patience=4,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "logger_callback = callbacks.CSVLogger(\n",
        "    filename=\"model_logs/\" + filename + '.csv',\n",
        "    separator=',',\n",
        "    append=True #don't overwrite if file exists already\n",
        ")\n",
        "\n",
        "#for periodically saving model\n",
        "checkpoint_path = 'checkpoints/' + filename\n",
        "checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir='tb_logs',\n",
        "    histogram_freq=2,\n",
        "    write_graph=True,\n",
        "    write_images=False,\n",
        "    write_steps_per_second=False,\n",
        "    update_freq='epoch',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#filename = \"inceptronv3_firstpass_retry_finetuned2_model\"\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, \n",
        "    y_train, \n",
        "    epochs=num_epochs, \n",
        "    batch_size=batch_size,  #32 works well\n",
        "    validation_split=0.2, \n",
        "    validation_batch_size=batch_size,\n",
        "    shuffle=True, #shuffles data after every epoch to reduce overfitting\n",
        "    callbacks=[logger_callback, tb_callback, checkpoint_callback, early_stopping_callback], # checkpoint_callback\n",
        "    #should use mult proccing/threading?\n",
        "    workers=4,\n",
        "    use_multiprocessing = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#history of training and validation accuracy (only works if model trained and not preloaded)\n",
        "plt.plot(history.history['accuracy'], label='training acc')\n",
        "plt.plot(history.history['val_accuracy'], label='validation acc')\n",
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#filename = \"inceptronv3_firstpass_retry_finetuned2_model\"\n",
        "\n",
        "## SAVE THE MODEL\n",
        "model.save(\"saved_models/\" + filename)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation\n",
        "## Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HdsbyQRHj6H"
      },
      "outputs": [],
      "source": [
        "## Model Prediction\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#apply preprocessing to x_test too (results in skewed output all 1s)\n",
        "#x_test = preprocc_input(x_test)\n",
        "\n",
        "#test prediction storage\n",
        "y_test_pred = model.predict(\n",
        "        x_test, \n",
        "        batch_size=batch_size,\n",
        "        workers=4,\n",
        "        use_multiprocessing = True,\n",
        "    ).flatten()\n",
        "\n",
        "#round probabilities to 0 or 1\n",
        "y_test_pred = np.round(y_test_pred)\n",
        "\n",
        "#for translating old preds to new format (requires manual insertion of 1st row)\n",
        "#df=pd.read_csv('predictions/' + 'HW4_predictions_SethCram_inceptronv3_dropout3_finetuned2.1_model.csv', sep = ',')\n",
        "#print(df)\n",
        "\n",
        "with open('predictions/' + 'HW4_predictions_SethCram' + '_' + filename + '.csv', 'w') as file:\n",
        "    writer = csv.writer(file, lineterminator='\\n')\n",
        "    \n",
        "    for i in range(1248):\n",
        "        if i == 0:\n",
        "            writeList = [\"\", \"name\", \"pred\"]\n",
        "        else:\n",
        "            writeList = [i, \"testImgs\\\\\" + str(i-1) + \".png\", y_test_pred[i-1]]\n",
        "\n",
        "            #translate old pred into new format\n",
        "            #writeList = [i, \"testImgs\\\\\" + df.values[i-1][0], df.values[i-1][1]]\n",
        "        \n",
        "        writer.writerow(writeList)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train accuracy\n",
        "y_train_pred = model.predict(\n",
        "        x_train, \n",
        "        batch_size=batch_size,\n",
        "        #workers=4,\n",
        "        #use_multiprocessing = True,\n",
        "    ).flatten()\n",
        "\n",
        "#round probabilities to 0 or 1\n",
        "y_train_pred = np.round(y_train_pred)\n",
        "\n",
        "train_acc = np.sum(y_train.flatten() == y_train_pred) / y_train.shape[0]\n",
        "\n",
        "print(f'train accuracy: {train_acc * 100}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss0, accuracy0 = model.evaluate(x_train, y_train, batch_size=batch_size,)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('JupyterNotebook': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7e1b2f627b942c46ec4bf47ff4ea2a9cdb5031d2bd74349327a6cc0e56088180"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
