{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2MIMrBnHj6D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import applications\n",
        "from keras import losses\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2 as cv\n",
        "import csv\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "print(\"Modules imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Colab\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/SethCram/CS474-Deep-Learning/blob/main/HW4/HW4_CNN_Cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dQmHIT2Mtvz",
        "outputId": "09cc133b-10f4-444f-915d-a129dc8b00dd"
      },
      "outputs": [],
      "source": [
        "## Google Colab Cell\n",
        "\n",
        "#enable debugging\n",
        "!pip install -Uqq ipdb\n",
        "import ipdb\n",
        "%pdb on\n",
        "\n",
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/School/Senior Year/CS 474-01 (Deep Learning)/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi3lNvXkHj6E",
        "outputId": "59eb91eb-b9e2-49d9-9a6d-e01836dffe24"
      },
      "outputs": [],
      "source": [
        "#LOAD CSV AND IMAGES (in proper resolution)\n",
        "\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/SethCram/CS474-Deep-Learning/main/HW4/train.csv', sep = ',')\n",
        "print(df)\n",
        "\n",
        "training_imgs_path = 'trainImgs/'\n",
        "test_imgs_path = 'testImgs/'\n",
        "\n",
        "targetWidth = 512\n",
        "targetHeight = 512\n",
        "\n",
        "# store training and test imgs greyscaled \n",
        "x_train = np.array([\n",
        "            cv.resize( #resize to desired size\n",
        "                cv.imread(training_imgs_path + row[1] + \".png\"), \n",
        "                dsize=(targetWidth, targetHeight),\n",
        "            ) \n",
        "            for row in df.values\n",
        "    ])\n",
        "x_test = np.array([\n",
        "            cv.resize( #resize to desired size\n",
        "                cv.imread(test_imgs_path + str(i) + \".png\"),\n",
        "                dsize=(targetWidth, targetHeight),\n",
        "            )\n",
        "            for i in range(1248)\n",
        "    ])\n",
        "\n",
        "#store whether benign (0?) or malignant (1?)\n",
        "y_train = np.array([row[2] for row in df.values], dtype='float32')\n",
        "\n",
        "nImg = 4  \n",
        "for i in range(nImg*nImg):\n",
        "    plt.subplot(nImg, nImg, i+1)\n",
        "    plt.imshow(x_train[i], cmap = 'Greys_r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transfer Learning\n",
        "## Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VGG Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## VGG\n",
        "\n",
        "#avg = 530\n",
        "#avg_training_width = sum( image.shape[0] for image in x_train ) / x_train.shape[0]\n",
        "#avg_training_height = sum( image.shape[1] for image in x_train ) / x_train.shape[0]\n",
        "#avg_test_width = sum( image.shape[0] for image in x_test ) / x_test.shape[0]\n",
        "#avg_test_height = sum( image.shape[1] for image in x_train ) / x_train.shape[0]\n",
        "\n",
        "# layer construction\n",
        "input_layer = layers.Input(shape=(targetWidth, targetHeight, 3))\n",
        "preprocc_input = applications.vgg16.preprocess_input\n",
        "base_model = applications.VGG16(\n",
        "    input_shape=(targetWidth, targetHeight, 3),\n",
        "    include_top=False,\n",
        ")\n",
        "flatten_layer = layers.Flatten()\n",
        "fc_layer = layers.Dense(512, activation='relu')\n",
        "# Add a dropout rate of 0.5\n",
        "#dropout_layer = layers.Dropout(0.5)\n",
        "output_layer = layers.Dense(\n",
        "    units=1,\n",
        "    activation='sigmoid'\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "#layer connecting\n",
        "x = preprocc_input(input_layer)\n",
        "x = base_model(x, training=False)\n",
        "x = flatten_layer(x)\n",
        "x = fc_layer(x)\n",
        "#x = dropout_layer(x)\n",
        "predictions = output_layer(x)\n",
        "model = keras.Model(input_layer, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### InceptionV3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# layer construction\n",
        "input_layer = layers.Input(shape=(targetWidth, targetHeight, 3))\n",
        "preprocc_input = applications.inception_v3.preprocess_input\n",
        "\n",
        "base_model = applications.InceptionV3(\n",
        "    input_shape=(targetWidth, targetHeight, 3),\n",
        "    include_top=False,\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "global_avg_pool_layer = layers.GlobalAveragePooling2D()\n",
        "fc_layer = layers.Dense(1024, activation='relu')\n",
        "#dropout_layer = layers.Dropout(0.2)\n",
        "output_layer = layers.Dense(\n",
        "    units=1,\n",
        "    activation=keras.activations.sigmoid #'softmax'\n",
        ")\n",
        "\n",
        "#layer connecting\n",
        "x = preprocc_input(input_layer)\n",
        "x = base_model(x, training=False)\n",
        "x = global_avg_pool_layer(x)\n",
        "x = fc_layer(x)\n",
        "#x = dropout_layer(x)\n",
        "predictions = output_layer(x)\n",
        "model = keras.Model(input_layer, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Efficient Net V2M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## DOESN'T WORK\n",
        "\n",
        "# layer construction\n",
        "input_layer = layers.Input(shape=(targetWidth, targetHeight, 3))\n",
        "preprocc_input = applications.efficientnet_v2.preprocess_input\n",
        "\n",
        "base_model = applications.EfficientNetV2B0( #V2M(\n",
        "    input_shape=(targetWidth, targetHeight, 3),\n",
        "    include_top=False,\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "fc_layer = layers.Dense(1024, activation='relu')\n",
        "dropout_layer = layers.Dropout(0.5) #0.5\n",
        "output_layer = layers.Dense(\n",
        "    units=1,\n",
        "    activation=keras.activations.sigmoid \n",
        ")\n",
        "\n",
        "#layer connecting\n",
        "x = preprocc_input(input_layer)\n",
        "x = base_model(x, training=False)\n",
        "x = fc_layer(x)\n",
        "x = dropout_layer(x)\n",
        "predictions = output_layer(x)\n",
        "model = keras.Model(input_layer, predictions)\n",
        "\n",
        "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.RMSprop(), #'adam', #to deal with noise\n",
        "    loss=losses.BinaryCrossentropy(), #or just categorical_crossentropy?\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#model.summary(expand_nested=True, show_trainable=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Restore Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## RESTORE THE MODEL\n",
        "\n",
        "model = keras.models.load_model(\"saved_models/inceptronv3_dropout2_model\")\n",
        "\n",
        "model.summary(expand_nested=True, show_trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 249 layers and unfreeze the rest:\n",
        "\n",
        "#base_model = model.layers[3]\n",
        "\n",
        "model.layers[3].trainable = True\n",
        "\n",
        "model.layers[3].layers[299].trainable = True\n",
        "\n",
        "for layer in model.layers[3].layers[:299]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[3].layers[300:]:\n",
        "   layer.trainable = False\n",
        "   \n",
        "model.layers[3].layers[294].trainable = True\n",
        "\n",
        "model.summary(expand_nested=True, show_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#need to recompile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.SGD(learning_rate=0.0001, momentum=0.9), #not sure why use SGD w/ mom\n",
        "    loss=losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tf.config.run_functions_eagerly(True)\n",
        "\n",
        "#shuffle data everytime train\n",
        "x_train, y_train = shuffle(x_train, y_train)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, #took first 500 for VGG\n",
        "    y_train, #took first 500 for VGG\n",
        "    epochs=10, #5, \n",
        "    batch_size=batch_size,  #128, 64 crashed it; 40, 20 in both Epoch ETA is 10 mins didn't crash it (VGG)\n",
        "    validation_split=0.2, \n",
        "    validation_batch_size=batch_size\n",
        ")\n",
        "\n",
        "#history of training and validation accuracy (only works if model trained and not preloaded)\n",
        "plt.plot(history.history['accuracy'], label='training acc')\n",
        "plt.plot(history.history['val_accuracy'], label='validation acc')\n",
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## SAVE THE MODEL\n",
        "\n",
        "model.save(\"saved_models/inceptronv3_dropout3_finetuned_model\")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HdsbyQRHj6H"
      },
      "outputs": [],
      "source": [
        "## Model Prediction\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#test prediction storage\n",
        "y_test_pred = model.predict(x_test, batch_size=batch_size).flatten()\n",
        "\n",
        "#round probabilities to 0 or 1\n",
        "y_test_pred = np.round(y_test_pred)\n",
        "\n",
        "with open('HW4_predictions_SethCram.csv', 'w') as file:\n",
        "    writer = csv.writer(file, lineterminator='\\n')\n",
        "    \n",
        "    for i in range(len(y_test_pred)):\n",
        "        writer.writerow([str(i) + \".png\", y_test_pred[i]])\n",
        "        \n",
        "#train accuracy\n",
        "y_train_pred = model.predict(x_train, batch_size=batch_size).flatten()\n",
        "\n",
        "#round probabilities to 0 or 1\n",
        "y_train_pred = np.round(y_train_pred)\n",
        "\n",
        "train_acc = np.sum(y_train == y_train_pred) / y_train.shape[0]\n",
        "\n",
        "print(f'train accuracy: {train_acc * 100}%')\n",
        "\n",
        "#apply preprocessing to x_test too\n",
        "x_test = preprocc_input(x_test)\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('JupyterNotebook': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7e1b2f627b942c46ec4bf47ff4ea2a9cdb5031d2bd74349327a6cc0e56088180"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
