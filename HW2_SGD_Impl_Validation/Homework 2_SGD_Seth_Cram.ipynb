{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2: Wine Quality Prediction Using SGD\n",
    "### Editor: Seth Cram\n",
    "### Course: CS 474/574: Deep Learning/2022 Fall\n",
    "### Due: 09/25/2022\n",
    "\n",
    "\n",
    "Add your code to the following sections:\n",
    "\n",
    "    ## add your code here\n",
    "    #-----------------------\n",
    "\n",
    "    #---------------------------------\n",
    "    \n",
    "Description: In this homework, you are going to practice cross-validation and implement the stochastic gradient optimization (mini-batch) to solve the wine quality prediction problem. Using the following code as your template. Specific requirements:\n",
    "\n",
    "1. Use all function definitions given in the code (e.g., def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100):); and do not change the function names and input arguments. (deduct 5 points for doing this)\n",
    "\n",
    "2. Evaluate (Cross-validation) the model trained using GD (20 points)\n",
    "\n",
    "3. SGD implementation. 40 pts\n",
    "   \n",
    "4. Calculate and print out the MSE and MAE values of SGD for the training and test sets (15 points)\n",
    "5. Plot the loss curve of the SGD. (5 points)\n",
    "6. Plot the mse curves on the training and test sets using different models (w_hist). (20 points)\n",
    "\n",
    "### Common mistakes\n",
    "    \n",
    "1. Call GD and SGD using the whole dataset\n",
    "\n",
    "    -- GD and SGD are used to optimize the model (learn w); and we should call them using the training sets\n",
    "   \n",
    "2. Calculate gradient using the whole training set for SGD\n",
    "    \n",
    "    -- In SGD, update gradient only using mini-batches\n",
    "  \n",
    "3. Calculate the loss of each epoch using the average of all minibatches\n",
    "    \n",
    "    -- should use the w of the last mini-batch and the whole training set to calculate the loss  \n",
    "   \n",
    "4. Mix concepts of loss function and evaulation metrics\n",
    "    -- loss function: for optimization purpose (gradient). We use the sum of square errors in this homework. L = 1/2 * sum(y_hat_i - y_i)^2\n",
    "    \n",
    "    -- evaluation metrics: mse and mae: mse = 1/m * sum(y_hat_i - y_i)^2, mae = 1/m * sum(abs(y_hat_i - y_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data, implement the model, loss function and GD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/HW2_SGD_Impl_Validation/winequality-white.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## (1) Data preparation\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/HW2_SGD_Impl_Validation/winequality-white.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m df\n\u001b[0;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mvalues[:, :\u001b[38;5;241m11\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m     f,\n\u001b[0;32m   1218\u001b[0m     mode,\n\u001b[0;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1225\u001b[0m )\n\u001b[0;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\crazy\\source\\repos\\CS474-Deep-Learning\\JupyterNotebook\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/HW2_SGD_Impl_Validation/winequality-white.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## (1) Data preparation\n",
    "df=pd.read_csv('winequality-white.csv', sep = ';')\n",
    "df\n",
    "X = df.values[:, :11]\n",
    "Y = df.values[:, 11]\n",
    "print('Data shape:', 'X:', X.shape, 'Y:', Y.shape)\n",
    "\n",
    "# data normalization\n",
    "min_vals = np.min(X, axis = 0)\n",
    "max_vals = np.max(X, axis = 0)\n",
    "X1 = (X-min_vals)/(max_vals-min_vals)\n",
    "\n",
    "##(2) Assume a linear mode that y = w0*1 + w_1*x_1 +w_2*x_2+...+ w_11*x_11\n",
    "def predict(X, w):\n",
    "    '''\n",
    "    X: input feature vectors:m*n\n",
    "    w: weights\n",
    "    \n",
    "    return Y_hat\n",
    "    '''\n",
    "    # Prediction\n",
    "    Y_hat = np.zeros((X.shape[0]))\n",
    "    for idx, x in enumerate(X):          \n",
    "        y_hat = w[0] + np.dot(w[1:].T, np.c_[x]) # linear model\n",
    "        Y_hat[idx] = y_hat    \n",
    "    return Y_hat\n",
    "\n",
    "## (3) Loss function: L = 1/2 * sum(y_hat_i - y_i)^2\n",
    "def loss(w, X, Y):\n",
    "    '''\n",
    "    w: weights\n",
    "    X: input feature vectors\n",
    "    Y: targets\n",
    "    '''\n",
    "    Y_hat = predict(X, w)\n",
    "    loss = 1/2* np.sum(np.square(Y - Y_hat))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Optimization 1: Gradient Descent\n",
    "def GD(X, Y, lr = 0.001, delta = 0.01, max_iter = 100):\n",
    "    '''\n",
    "    X: training data\n",
    "    Y: training target\n",
    "    lr: learning rate\n",
    "    max_iter: the max iterations\n",
    "    '''\n",
    "    \n",
    "    m = len(Y)\n",
    "    b = np.reshape(Y, [Y.shape[0],1])\n",
    "    w = np.random.rand(X.shape[1] + 1, 1)\n",
    "    A = np.c_[np.ones((m, 1)), X]\n",
    "    gradient = A.T.dot(np.dot(A, w)-b)\n",
    "    \n",
    "    loss_hist = np.zeros(max_iter) # history of loss\n",
    "    w_hist = np.zeros((max_iter, w.shape[0])) # history of weight\n",
    "    loss_w = 0\n",
    "    i = 0                  \n",
    "    while(np.linalg.norm(gradient) > delta) and (i < max_iter):\n",
    "        w_hist[i,:] = w.T\n",
    "        loss_w = loss(w, X, Y)\n",
    "        print(i, 'loss:', loss_w)\n",
    "        loss_hist[i] = loss_w\n",
    "        \n",
    "        w = w - lr*gradient        \n",
    "        gradient = A.T.dot(np.dot(A, w)-b) # update the gradient using new w\n",
    "        i = i + 1\n",
    "        \n",
    "    w_star = w  \n",
    "    return w_star, loss_hist, w_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model evaluation using cross-validation (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3428, 11) (1470, 11)\n"
     ]
    }
   ],
   "source": [
    "## 2.1 Split the dataset into training (70%) and test (30%) sets. (5 points)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "#print(X_train.shape, X_test.shape)\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 2.2 Model training using the training set and the GD function (5 points )\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mse: 0.7867769198855628 and training mae:0.6927334108062077\n",
      "test mse: 0.8391500534666578 and test mae:0.714897056700805\n"
     ]
    }
   ],
   "source": [
    "## 2.3. calculating mse&mae values on the training set and test set, respectively. (10 points)\n",
    "\n",
    "#training error\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "#print('training mse: {} and training mae:{}'.format(mse_train, mae_train))\n",
    "#---------------------------------\n",
    "\n",
    "\n",
    "## test error\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "#print('test mse: {} and test mae:{}'.format(mse_test, mae_test))\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SGD implementation (40 points)\n",
    "Use the SGD function definition given in the code (def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100):); and do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100): \n",
    "    '''Implement the minibatch Gradient Desent approach\n",
    "    \n",
    "        X: training data\n",
    "        Y: training target\n",
    "        lr: learning rate\n",
    "        batch_size: batch size\n",
    "        epoch: number of max epoches\n",
    "        \n",
    "        return: w_star, w_hist, loss_hist\n",
    "    '''\n",
    "    m = len(Y)\n",
    "    np.random.seed(9)\n",
    "    w = np.random.rand(X.shape[1]+1, 1)    #(12,1) values in [0, 1)\n",
    "    w_hist = np.zeros((epoch, w.shape[0])) # (epoch,12) \n",
    "    loss_hist = np.zeros(epoch)            # (epoch,)\n",
    "   \n",
    "    \n",
    "    ## add your code here\n",
    "    #-----------------------\n",
    "    for i in range(epoch):\n",
    "        #(1) Shuffle data (X and Y) at the beginning of each epoch. (5 points)\n",
    "\n",
    "        \n",
    "        #(2) go through all minibatches and update w. (30 points)\n",
    "        for b in range(int(m/batch_size)): \n",
    "            # prepare the b mininath X_batch and Y_batch. 10 points\n",
    "\n",
    "            \n",
    "            #prepare A_batch and b_batch. 10 points\n",
    "\n",
    "            \n",
    "            #gradient calcualation and w update. 10 points\n",
    "            #print(i, b, X_batch.shape, A_batch.shape)\n",
    "\n",
    "            \n",
    "            \n",
    "        ## (3) Save the loss and current weight for each epoch. 5 points\n",
    "\n",
    "        \n",
    "        #print(i, loss_hist[i])\n",
    "        \n",
    "        ##(4) Decay learning rate at the end of each epoch. \n",
    "        lr = lr * 0.9\n",
    "    #---------------------------------\n",
    "    \n",
    "    w_star = w\n",
    "    return w_star, w_hist, loss_hist  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate and print out the MSE and MAE values of SGD for the training and test sets (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [6.  5.6 5.7 6.5 5.7 7.  6.7 6.2 5.7 5.8]\n",
      "True [5. 6. 7. 8. 5. 4. 6. 5. 7. 5.]\n",
      "training mse: 0.7218402431182253 and training mae:0.6595991336722727\n",
      "test mse: 0.7735707034736699 and test mae:0.6839863502607874\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "#train model using SGD\n",
    "w_star_SGD, w_hist_SGD, loss_hist_SGD = SGD(X_train, y_train, lr = 0.0001, batch_size = batch_size, epoch = n_epochs)\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "#(1) print out the predicted wine quality values and the true quality \n",
    "# values of the first 10 data samples in the test dataset.  5 points\n",
    "\n",
    "\n",
    "\n",
    "#(2) mse and mae of the training set. 5 points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(3)mse and mae of the test set. 5 points\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the loss curve of the SGD. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot the mse curves on the training and test sets using different models (w_hist). (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_SGD_train=np.zeros(n_epochs)\n",
    "mse_SGD_test=np.zeros(n_epochs)\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('JupyterNotebook': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1b2f627b942c46ec4bf47ff4ea2a9cdb5031d2bd74349327a6cc0e56088180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
