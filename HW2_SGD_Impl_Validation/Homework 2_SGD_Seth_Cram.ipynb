{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2: Wine Quality Prediction Using SGD\n",
    "### Editor: Seth Cram\n",
    "### Course: CS 474/574: Deep Learning/2022 Fall\n",
    "### Due: 09/25/2022\n",
    "\n",
    "\n",
    "Add your code to the following sections:\n",
    "\n",
    "    ## add your code here\n",
    "    #-----------------------\n",
    "\n",
    "    #---------------------------------\n",
    "    \n",
    "Description: In this homework, you are going to practice cross-validation and implement the stochastic gradient optimization (mini-batch) to solve the wine quality prediction problem. Using the following code as your template. Specific requirements:\n",
    "\n",
    "1. Use all function definitions given in the code (e.g., def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100):); and do not change the function names and input arguments. (deduct 5 points for doing this)\n",
    "\n",
    "2. Evaluate (Cross-validation) the model trained using GD (20 points)\n",
    "\n",
    "3. SGD implementation. 40 pts\n",
    "   \n",
    "4. Calculate and print out the MSE and MAE values of SGD for the training and test sets (15 points)\n",
    "5. Plot the loss curve of the SGD. (5 points)\n",
    "6. Plot the mse curves on the training and test sets using different models (w_hist). (20 points)\n",
    "\n",
    "### Common mistakes\n",
    "    \n",
    "1. Call GD and SGD using the whole dataset\n",
    "\n",
    "    -- GD and SGD are used to optimize the model (learn w); and we should call them using the training sets\n",
    "   \n",
    "2. Calculate gradient using the whole training set for SGD\n",
    "    \n",
    "    -- In SGD, update gradient only using mini-batches\n",
    "  \n",
    "3. Calculate the loss of each epoch using the average of all minibatches\n",
    "    \n",
    "    -- should use the w of the last mini-batch and the whole training set to calculate the loss  \n",
    "   \n",
    "4. Mix concepts of loss function and evaulation metrics\n",
    "    -- loss function: for optimization purpose (gradient). We use the sum of square errors in this homework. L = 1/2 * sum(y_hat_i - y_i)^2\n",
    "    \n",
    "    -- evaluation metrics: mse and mae: mse = 1/m * sum(y_hat_i - y_i)^2, mae = 1/m * sum(abs(y_hat_i - y_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data, implement the model, loss function and GD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: X: (4898, 11) Y: (4898,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## (1) Data preparation\n",
    "df=pd.read_csv('winequality-white.csv', sep = ';')\n",
    "df\n",
    "X = df.values[:, :11]\n",
    "Y = df.values[:, 11]\n",
    "print('Data shape:', 'X:', X.shape, 'Y:', Y.shape)\n",
    "\n",
    "# data normalization\n",
    "min_vals = np.min(X, axis = 0)\n",
    "max_vals = np.max(X, axis = 0)\n",
    "X1 = (X-min_vals)/(max_vals-min_vals)\n",
    "\n",
    "##(2) Assume a linear mode that y = w0*1 + w_1*x_1 +w_2*x_2+...+ w_11*x_11\n",
    "def predict(X, w):\n",
    "    '''\n",
    "    X: input feature vectors:m*n\n",
    "    w: weights\n",
    "    \n",
    "    return Y_hat\n",
    "    '''\n",
    "    # Prediction\n",
    "    Y_hat = np.zeros((X.shape[0]))\n",
    "    for idx, x in enumerate(X):          \n",
    "        y_hat = w[0] + np.dot(w[1:].T, np.c_[x]) # linear model\n",
    "        Y_hat[idx] = y_hat    \n",
    "    return Y_hat\n",
    "\n",
    "## (3) Loss function: L = 1/2 * sum(y_hat_i - y_i)^2\n",
    "def loss(w, X, Y):\n",
    "    '''\n",
    "    w: weights\n",
    "    X: input feature vectors\n",
    "    Y: targets\n",
    "    '''\n",
    "    Y_hat = predict(X, w)\n",
    "    loss = 1/2* np.sum(np.square(Y - Y_hat))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Optimization 1: Gradient Descent\n",
    "def GD(X, Y, lr = 0.001, delta = 0.01, max_iter = 100):\n",
    "    '''\n",
    "    X: training data\n",
    "    Y: training target\n",
    "    lr: learning rate\n",
    "    max_iter: the max iterations\n",
    "    '''\n",
    "    \n",
    "    m = len(Y)\n",
    "    b = np.reshape(Y, [Y.shape[0],1])\n",
    "    w = np.random.rand(X.shape[1] + 1, 1)\n",
    "    A = np.c_[np.ones((m, 1)), X]\n",
    "    gradient = A.T.dot(np.dot(A, w)-b)\n",
    "    \n",
    "    loss_hist = np.zeros(max_iter) # history of loss\n",
    "    w_hist = np.zeros((max_iter, w.shape[0])) # history of weight\n",
    "    loss_w = 0\n",
    "    i = 0                  \n",
    "    while(np.linalg.norm(gradient) > delta) and (i < max_iter):\n",
    "        w_hist[i,:] = w.T\n",
    "        loss_w = loss(w, X, Y)\n",
    "        print(i, 'loss:', loss_w)\n",
    "        loss_hist[i] = loss_w\n",
    "        \n",
    "        w = w - lr*gradient        \n",
    "        gradient = A.T.dot(np.dot(A, w)-b) # update the gradient using new w\n",
    "        i = i + 1\n",
    "        \n",
    "    w_star = w  \n",
    "    return w_star, loss_hist, w_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model evaluation using cross-validation (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3428, 11) (1470, 11)\n",
      "(3428,) (1470,)\n"
     ]
    }
   ],
   "source": [
    "## 2.1 Split the dataset into training (70%) and test (30%) sets. (5 points)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "#need to run above cell atleast once to get X1\n",
    "\n",
    "#input dataset is X (should also split Y-resultant dataset to get training and test targets)\n",
    "\n",
    "# data split of 70 training and 30 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y, test_size=0.3)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 31039.828301026897\n",
      "1 loss: 747079.1388106726\n",
      "2 loss: 18718320.26520528\n",
      "3 loss: 469736407.84269536\n",
      "4 loss: 11788762349.895407\n",
      "5 loss: 295857948002.578\n",
      "6 loss: 7425031650693.167\n",
      "7 loss: 186343127125300.12\n",
      "8 loss: 4676580877415084.0\n",
      "9 loss: 1.1736632866755728e+17\n",
      "10 loss: 2.9454970342602127e+18\n",
      "11 loss: 7.392199174441771e+19\n",
      "12 loss: 1.8551914328558168e+21\n",
      "13 loss: 4.655901675973882e+22\n",
      "14 loss: 1.1684735080393802e+24\n",
      "15 loss: 2.9324724489682594e+25\n",
      "16 loss: 7.359511880065732e+26\n",
      "17 loss: 1.84698802990919e+28\n",
      "18 loss: 4.635313915136125e+29\n",
      "19 loss: 1.1633066778949832e+31\n",
      "20 loss: 2.9195054566122535e+32\n",
      "21 loss: 7.326969124437684e+33\n",
      "22 loss: 1.8388209012891432e+35\n",
      "23 loss: 4.6148171905628364e+36\n",
      "24 loss: 1.1581626947672772e+38\n",
      "25 loss: 2.9065958025241035e+39\n",
      "26 loss: 7.2945702684270525e+40\n",
      "27 loss: 1.8306898865955646e+42\n",
      "28 loss: 4.594411099704102e+43\n",
      "29 loss: 1.1530414576298782e+45\n",
      "30 loss: 2.893743233161829e+46\n",
      "31 loss: 7.262314675729394e+47\n",
      "32 loss: 1.8225948261376048e+49\n",
      "33 loss: 4.57409524179002e+50\n",
      "34 loss: 1.1479428659031251e+52\n",
      "35 loss: 2.8809474961045724e+53\n",
      "36 loss: 7.230201712853912e+54\n",
      "37 loss: 1.814535560930548e+56\n",
      "38 loss: 4.553869217822835e+57\n",
      "39 loss: 1.1428668194521048e+59\n",
      "40 loss: 2.8682083400476445e+60\n",
      "41 loss: 7.19823074911103e+61\n",
      "42 loss: 1.8065119326926875e+63\n",
      "43 loss: 4.5337326305691194e+64\n",
      "44 loss: 1.1378132185846924e+66\n",
      "45 loss: 2.8555255147976006e+67\n",
      "46 loss: 7.166401156599995e+68\n",
      "47 loss: 1.798523783842217e+70\n",
      "48 loss: 4.513685084551953e+71\n",
      "49 loss: 1.1327819640495842e+73\n",
      "50 loss: 2.842898771267311e+74\n",
      "51 loss: 7.1347123101965465e+75\n",
      "52 loss: 1.790570957494136e+77\n",
      "53 loss: 4.493726186043155e+78\n",
      "54 loss: 1.1277729570343535e+80\n",
      "55 loss: 2.8303278614710755e+81\n",
      "56 loss: 7.103163587540623e+82\n",
      "57 loss: 1.7826532974571645e+84\n",
      "58 loss: 4.473855543055558e+85\n",
      "59 loss: 1.1227860991635082e+87\n",
      "60 loss: 2.817812538519757e+88\n",
      "61 loss: 7.071754369024185e+89\n",
      "62 loss: 1.7747706482306846e+91\n",
      "63 loss: 4.4540727653353174e+92\n",
      "64 loss: 1.1178212924965594e+94\n",
      "65 loss: 2.805352556615922e+95\n",
      "66 loss: 7.040484037778977e+96\n",
      "67 loss: 1.766922855001677e+98\n",
      "68 loss: 4.434377464354227e+99\n",
      "69 loss: 1.1128784395260975e+101\n",
      "70 loss: 2.7929476710490236e+102\n",
      "71 loss: 7.009351979664477e+103\n",
      "72 loss: 1.759109763641681e+105\n",
      "73 loss: 4.414769253302099e+106\n",
      "74 loss: 1.107957443175876e+108\n",
      "75 loss: 2.7805976381905887e+109\n",
      "76 loss: 6.978357583255796e+110\n",
      "77 loss: 1.7513312207037794e+112\n",
      "78 loss: 4.395247747079173e+113\n",
      "79 loss: 1.1030582067989084e+115\n",
      "80 loss: 2.7683022154894368e+116\n",
      "81 loss: 6.947500239831684e+117\n",
      "82 loss: 1.743587073419568e+119\n",
      "83 loss: 4.3758125622885413e+120\n",
      "84 loss: 1.0981806341755667e+122\n",
      "85 loss: 2.7560611614669207e+123\n",
      "86 loss: 6.916779343362592e+124\n",
      "87 loss: 1.735877169696173e+126\n",
      "88 loss: 4.356463317228644e+127\n",
      "89 loss: 1.0933246295116958e+129\n",
      "90 loss: 2.743874235712174e+130\n",
      "91 loss: 6.88619429049872e+131\n",
      "92 loss: 1.7282013581132432e+133\n",
      "93 loss: 4.337199631885718e+134\n",
      "94 loss: 1.0884900974367231e+136\n",
      "95 loss: 2.7317411988773906e+137\n",
      "96 loss: 6.85574448055821e+138\n",
      "97 loss: 1.7205594879199949e+140\n",
      "98 loss: 4.3180211279264004e+141\n",
      "99 loss: 1.0836769430017973e+143\n",
      "(12, 1) (100,) (100, 12)\n"
     ]
    }
   ],
   "source": [
    "## 2.2 Model training using the training set and the GD function (5 points )\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "w_star, loss_hist, w_hist = GD(X_train, y_train)\n",
    "\n",
    "print(w_star.shape, loss_hist.shape, w_hist.shape)\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mse: 1.5867338463670562e+141 and training mae:3.976153775443393e+70\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1470,11) (12,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 43\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining mse: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and training mae:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mse_train, mae_train))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#---------------------------------\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m## test error\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m## add your code here\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#-----------------------\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m y_test_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_star\u001b[49m\n\u001b[0;32m     45\u001b[0m mse_test \u001b[38;5;241m=\u001b[39m mean_squared_error(y_true\u001b[38;5;241m=\u001b[39my_test, y_pred\u001b[38;5;241m=\u001b[39my_test_prediction )\n\u001b[0;32m     46\u001b[0m mae_test \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_true\u001b[38;5;241m=\u001b[39my_test, y_pred\u001b[38;5;241m=\u001b[39my_test_prediction )\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1470,11) (12,1) "
     ]
    }
   ],
   "source": [
    "## 2.3. calculating mse&mae values on the training set and test set, respectively. (10 points)\n",
    "\n",
    "#training error\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\"\"\"\n",
    "def MSE(Y, Y_hat):\n",
    "    Y_length = len(Y)\n",
    "    Y_hat_length = len(Y_hat)\n",
    "    \n",
    "    #ensure same num of ellys for both target and prediction vals\n",
    "    assert Y_length == Y_hat_length, \"{} target values but only {} prediction values\".format(Y_length, Y_hat_length)\n",
    "    \n",
    "    mse = \n",
    "    \n",
    "    return mse\n",
    "\"\"\"\n",
    "\n",
    "#Values are way off:\n",
    "# so maybe don't use w_star as wight??\n",
    "\n",
    "y_train_prediction = predict(X_train, w_star)\n",
    "\n",
    "#w_star = last elly of every w_hist\n",
    "# the resultant prediction from that GD run\n",
    "\n",
    "#mse_train = mean_squared_error(y_true=y_train, y_pred=w_star )\n",
    "mse_train = mean_squared_error(y_true=y_train, y_pred=y_train_prediction )\n",
    "mae_train = mean_absolute_error(y_true=y_train, y_pred=y_train_prediction )\n",
    "\n",
    "print('training mse: {} and training mae:{}'.format(mse_train, mae_train))\n",
    "#---------------------------------\n",
    "\n",
    "\n",
    "## test error\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "y_test_prediction = predict(X_test, w_star)\n",
    "\n",
    "mse_test = mean_squared_error(y_true=y_test, y_pred=y_test_prediction )\n",
    "mae_test = mean_absolute_error(y_true=y_test, y_pred=y_test_prediction )\n",
    "\n",
    "print('test mse: {} and test mae:{}'.format(mse_test, mae_test))\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SGD implementation (40 points)\n",
    "Use the SGD function definition given in the code (def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100):); and do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def SGD(X, Y, lr = 0.001, batch_size = 32, epoch = 100): \n",
    "    '''Implement the minibatch Gradient Desent approach\n",
    "    \n",
    "        X: training data\n",
    "        Y: training target\n",
    "        lr: learning rate\n",
    "        batch_size: batch size\n",
    "        epoch: number of max epoches\n",
    "        \n",
    "        return: w_star, w_hist, loss_hist\n",
    "    '''\n",
    "    m = len(Y)\n",
    "    np.random.seed(9)\n",
    "    w = np.random.rand(X.shape[1]+1, 1)    #(12,1) values in [0, 1)\n",
    "    w_hist = np.zeros((epoch, w.shape[0])) # (epoch,12) \n",
    "    loss_hist = np.zeros(epoch)            # (epoch,)\n",
    "   \n",
    "    \n",
    "    ## add your code here\n",
    "    #-----------------------\n",
    "    for i in range(epoch):\n",
    "        #(1) Shuffle data (X and Y) at the beginning of each epoch. (5 points)\n",
    "        Xshuffled = shuffle(X)\n",
    "        Yshuffled = shuffle(Y)\n",
    "        \n",
    "        #(2) go through all minibatches and update w. (30 points)\n",
    "        for b in range(int(m/batch_size)): \n",
    "            # prepare the b mininath X_batch and Y_batch. 10 points\n",
    "            \n",
    "            #excludes data that didn't fit in a batch on the end of X and Y\n",
    "            \n",
    "            batchPrevCutoffIndex = batch_size * b\n",
    "            batchCurrCutoffIndex = batch_size * (b+1)\n",
    "            \n",
    "            #print(\"Batch number is {} and the highest batch number is {}.\".format(b, int(m/batch_size)-1))\n",
    "            \n",
    "            #separate shuffled data into batches at the cutoff points within the data\n",
    "            X_batch = Xshuffled[batchPrevCutoffIndex:batchCurrCutoffIndex]\n",
    "            Y_batch = Yshuffled[batchPrevCutoffIndex:batchCurrCutoffIndex]\n",
    "            \n",
    "            #prepare A_batch and b_batch. 10 points\n",
    "\n",
    "            A_batch = np.c_[np.ones((batch_size, 1)), X_batch]\n",
    "            b_batch = np.reshape(Y_batch, [Y_batch.shape[0],1])\n",
    "            \n",
    "            #gradient calcualation and w update. 10 points\n",
    "            gradient = A_batch.T.dot(np.dot(A_batch, w)-b_batch) # update the gradient using new w and only batches\n",
    "            w = w - lr*gradient        \n",
    "            \n",
    "            #print(i, b, X_batch.shape, A_batch.shape)\n",
    "\n",
    "            \n",
    "            \n",
    "        ## (3) Save the loss and current weight for each epoch. 5 points\n",
    "        w_hist[i,:] = w.T\n",
    "        #use the w of the last mini-batch and whole training set to calc loss\n",
    "        loss_w = loss(w, X, Y)\n",
    "        #print(i, 'loss:', loss_w)\n",
    "        loss_hist[i] = loss_w\n",
    "        \n",
    "        #print(i, loss_hist[i])\n",
    "        \n",
    "        ##(4) Decay learning rate at the end of each epoch. \n",
    "        lr = lr * 0.9\n",
    "    #---------------------------------\n",
    "    \n",
    "    w_star = w\n",
    "    return w_star, w_hist, loss_hist  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate and print out the MSE and MAE values of SGD for the training and test sets (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 13503.096160735444\n",
      "1 loss: 5446.7423419938805\n",
      "2 loss: 2858.304064508651\n",
      "3 loss: 1933.865062735598\n",
      "4 loss: 1573.641765139685\n",
      "5 loss: 1419.9593512344945\n",
      "6 loss: 1349.5845296902366\n",
      "7 loss: 1313.7443755698246\n",
      "8 loss: 1295.1208919822848\n",
      "9 loss: 1284.85821050549\n",
      "10 loss: 1278.8074286352228\n",
      "11 loss: 1275.0165978930509\n",
      "12 loss: 1272.680182074274\n",
      "13 loss: 1271.1228483228158\n",
      "14 loss: 1270.0318607241834\n",
      "15 loss: 1269.3711793620732\n",
      "16 loss: 1268.860941991586\n",
      "17 loss: 1268.490650827181\n",
      "18 loss: 1268.2507442438064\n",
      "19 loss: 1268.095787119973\n",
      "20 loss: 1267.9527497461231\n",
      "21 loss: 1267.8697135818525\n",
      "22 loss: 1267.7988186244472\n",
      "23 loss: 1267.7542194975601\n",
      "24 loss: 1267.730017887076\n",
      "25 loss: 1267.6916734545134\n",
      "26 loss: 1267.671509591361\n",
      "27 loss: 1267.6582205023474\n",
      "28 loss: 1267.6483092892795\n",
      "29 loss: 1267.6363718875218\n",
      "30 loss: 1267.6353452699902\n",
      "31 loss: 1267.6177260103982\n",
      "32 loss: 1267.6171298123277\n",
      "33 loss: 1267.615934226963\n",
      "34 loss: 1267.610129388233\n",
      "35 loss: 1267.6083604712996\n",
      "36 loss: 1267.6044060345728\n",
      "37 loss: 1267.599237717593\n",
      "38 loss: 1267.6000753509547\n",
      "39 loss: 1267.5998120867207\n",
      "40 loss: 1267.600991040525\n",
      "41 loss: 1267.599407281631\n",
      "42 loss: 1267.598313140422\n",
      "43 loss: 1267.5964458627177\n",
      "44 loss: 1267.5947260091064\n",
      "45 loss: 1267.5953319554574\n",
      "46 loss: 1267.5940899476384\n",
      "47 loss: 1267.5942651422833\n",
      "48 loss: 1267.5923021179408\n",
      "49 loss: 1267.5925263731465\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n",
      "Predicted wine value = [-4.07855053e+70 -3.84813582e+70 -3.94783540e+70 ... -4.26971563e+70\n",
      " -3.85611163e+70 -4.21273765e+70], true wine value = [7. 5. 6. ... 7. 6. 6.]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "#train model using SGD\n",
    "w_star_SGD, w_hist_SGD, loss_hist_SGD = SGD(X_train, y_train, lr = 0.0001, batch_size = batch_size, epoch = n_epochs)\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "#(1) print out the predicted wine quality values and the true quality \n",
    "# values of the first 10 data samples in the test dataset.  5 points\n",
    "\n",
    "y_test_prediction = predict(X_test, w_star)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    #print(\"Predicted wine value = {}, true wine value = {}\".format(w_hist_SGD[i],w_star_SGD[i])) \n",
    "    #why is it 12 prediction vals for every 1 true val?\n",
    "    print(\"Predicted wine value = {}, true wine value = {}\".format(y_test_prediction, y_test))\n",
    "\n",
    "\n",
    "#(2) mse and mae of the training set. 5 points\n",
    "\n",
    "y_train_prediction = predict(X_train, w_star)\n",
    "\n",
    "mse_train = mean_squared_error(y_true=y_train, y_pred=y_train_prediction )\n",
    "mae_train = mean_absolute_error(y_true=y_train, y_pred=y_train_prediction )\n",
    "\n",
    "\n",
    "#(3)mse and mae of the test set. 5 points\n",
    "\n",
    "mse_test = mean_squared_error(y_true=y_test, y_pred=y_test_prediction )\n",
    "mae_test = mean_absolute_error(y_true=y_test, y_pred=y_test_prediction )\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the loss curve of the SGD. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "plt.hist(loss_hist_SGD)\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Occurences of loss')\n",
    "plt.show()\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot the mse curves on the training and test sets using different models (w_hist). (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_SGD_train=np.zeros(n_epochs)\n",
    "mse_SGD_test=np.zeros(n_epochs)\n",
    "\n",
    "## add your code here\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('JupyterNotebook': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1b2f627b942c46ec4bf47ff4ea2a9cdb5031d2bd74349327a6cc0e56088180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
